{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bee.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhwid/bee_detecion/blob/training_tiny_model/bee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "enWUd27FgVaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b56b6b4c-2973-4db8-c3bc-ec3584e44987"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf bee_detecion\n",
        "!git clone -b training_tiny_model https://github.com/dhwid/bee_detecion.git\n",
        "%cd bee_detecion"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bee_detecion'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 44 (delta 12), reused 42 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n",
            "/content/bee_detecion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n9y9ulIz5pC_",
        "colab_type": "code",
        "outputId": "fea104b0-e1c0-4cdf-ecfe-d285a9d5f89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3-tiny.weights"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert.py  main.py\ttrain.py\t   yolo3\t       yolov3-tiny.cfg\n",
            "dataset     model_data\tvoc_annotation.py  yolov3_tiny-1c.cfg\n",
            "--2018-12-28 20:42:05--  https://pjreddie.com/media/files/yolov3-tiny.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35434956 (34M) [application/octet-stream]\n",
            "Saving to: ‘yolov3-tiny.weights’\n",
            "\n",
            "yolov3-tiny.weights 100%[===================>]  33.79M  15.6MB/s    in 2.2s    \n",
            "\n",
            "2018-12-28 20:42:08 (15.6 MB/s) - ‘yolov3-tiny.weights’ saved [35434956/35434956]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbDDLM_GMRl0",
        "colab_type": "code",
        "outputId": "d4e84c85-05f0-4b69-8943-e1b8ef50eeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert.py  model_data\t       yolo3\t\t   yolov3-tiny.weights\n",
            "dataset     train.py\t       yolov3_tiny-1c.cfg\n",
            "main.py     voc_annotation.py  yolov3-tiny.cfg\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rLmTYM0pOmRG",
        "colab_type": "code",
        "outputId": "95a67ccb-26dc-47f7-8783-54b660c72c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2881
        }
      },
      "cell_type": "code",
      "source": [
        "!python convert.py -w yolov3_tiny-1c.cfg yolov3-tiny.weights model_data/tiny_yolo_weights.h5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert.py  model_data\t       yolo3\t\t   yolov3-tiny.weights\n",
            "dataset     train.py\t       yolov3_tiny-1c.cfg\n",
            "main.py     voc_annotation.py  yolov3-tiny.cfg\n",
            "Using TensorFlow backend.\n",
            "Loading weights.\n",
            "Weights Header:  0 2 0 [32013312]\n",
            "Parsing Darknet config.\n",
            "Creating Keras model.\n",
            "Parsing section net_0\n",
            "Parsing section convolutional_0\n",
            "conv2d bn leaky (3, 3, 3, 16)\n",
            "2018-12-28 20:42:56.633977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-12-28 20:42:56.634464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2018-12-28 20:42:56.634521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2018-12-28 20:42:57.554001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-12-28 20:42:57.554073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2018-12-28 20:42:57.554114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2018-12-28 20:42:57.554400: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2018-12-28 20:42:57.554525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Parsing section maxpool_0\n",
            "Parsing section convolutional_1\n",
            "conv2d bn leaky (3, 3, 16, 32)\n",
            "Parsing section maxpool_1\n",
            "Parsing section convolutional_2\n",
            "conv2d bn leaky (3, 3, 32, 64)\n",
            "Parsing section maxpool_2\n",
            "Parsing section convolutional_3\n",
            "conv2d bn leaky (3, 3, 64, 128)\n",
            "Parsing section maxpool_3\n",
            "Parsing section convolutional_4\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section maxpool_4\n",
            "Parsing section convolutional_5\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section maxpool_5\n",
            "Parsing section convolutional_6\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_7\n",
            "conv2d bn leaky (1, 1, 1024, 256)\n",
            "Parsing section convolutional_8\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_9\n",
            "conv2d    linear (1, 1, 512, 255)\n",
            "Parsing section yolo_0\n",
            "Parsing section route_0\n",
            "Parsing section convolutional_10\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section upsample_0\n",
            "Parsing section route_1\n",
            "Concatenating route layers: [<tf.Tensor 'up_sampling2d_1/ResizeNearestNeighbor:0' shape=(?, ?, ?, 128) dtype=float32>, <tf.Tensor 'leaky_re_lu_5/LeakyRelu:0' shape=(?, ?, ?, 256) dtype=float32>]\n",
            "Parsing section convolutional_11\n",
            "conv2d bn leaky (3, 3, 384, 256)\n",
            "Parsing section convolutional_12\n",
            "conv2d    linear (1, 1, 256, 30)\n",
            "Parsing section yolo_1\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 1 432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 1 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 3 4608        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 3 128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 3 0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 6 18432       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 6 0           leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 1 73728       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 2 294912      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 2 0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 5 1179648     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 5 2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 5 0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 1 4718592     max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 1 4096        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 2 262144      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 2 1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 3 0           up_sampling2d_1[0][0]            \n",
            "                                                                 leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 5 1179648     leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 2 884736      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 5 2048        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 3 7710        leaky_re_lu_11[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 8,800,909\n",
            "Trainable params: 8,794,541\n",
            "Non-trainable params: 6,368\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Saved Keras weights to model_data/tiny_yolo_weights.h5\n",
            "Read 8800909 of 8858734.0 from Darknet weights.\n",
            "Warning: 57825.0 unused weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sdwQ1QquOeGf",
        "colab_type": "code",
        "outputId": "b611f641-6a00-4d4d-80ce-14badcb4c126",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert.py  model_data\t       yolo3\t\t   yolov3-tiny.weights\n",
            "dataset     train.py\t       yolov3_tiny-1c.cfg\n",
            "main.py     voc_annotation.py  yolov3-tiny.cfg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-50c6d3a8-9570-4c17-ae8b-50eeffa1d56b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-50c6d3a8-9570-4c17-ae8b-50eeffa1d56b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.txt to train.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train.txt': b'dataset/train/img0001.jpg 348,320,403,412,0 \\ndataset/train/img0002.jpg 346,292,403,384,0 \\ndataset/train/img0004.jpg 312,247,394,304,0 290,314,348,380,0 339,300,415,350,0 346,338,418,388,0 \\ndataset/train/img0011.jpg 126,109,199,180,0 \\ndataset/train/img0012.jpg 258,289,336,358,0 \\ndataset/train/img0013.jpg 322,237,405,310,0 298,295,394,364,0 \\ndataset/train/img0018.jpg 236,308,322,401,0 \\ndataset/train/img0019.jpg 204,301,270,391,0 \\ndataset/train/img0020.jpg 108,342,162,412,0 \\ndataset/train/img0021.jpg 279,335,360,412,0 0,368,64,414,0 \\ndataset/train/img0022.jpg 252,285,339,366,0 \\ndataset/train/img0023.jpg 80,355,172,414,0 \\ndataset/train/img0024.jpg 119,362,207,414,0 \\ndataset/train/img0025.jpg 127,366,221,415,0 \\ndataset/train/img0026.jpg 145,359,238,415,0 \\ndataset/train/img0027.jpg 174,343,258,398,0 \\ndataset/train/img0028.jpg 211,274,309,347,0 \\ndataset/train/img0029.jpg 200,340,296,403,0 \\ndataset/train/img0030.jpg 199,334,294,393,0 \\ndataset/train/img0031.jpg 248,339,328,396,0 \\ndataset/train/img0032.jpg 122,105,212,191,0 259,332,341,390,0 \\ndataset/train/img0033.jpg 226,74,320,201,0 \\ndataset/train/img0035.jpg 179,263,270,338,0 \\ndataset/train/img0036.jpg 167,278,263,348,0 \\ndataset/train/img0037.jpg 172,282,248,374,0 \\ndataset/train/img0038.jpg 178,303,244,382,0 \\ndataset/train/img0039.jpg 180,307,246,383,0 \\ndataset/train/img0040.jpg 179,302,247,385,0 \\ndataset/train/img0041.jpg 152,307,233,378,0 \\ndataset/train/img0042.jpg 124,306,222,378,0 351,327,416,387,0 \\ndataset/train/img0043.jpg 64,296,162,383,0 \\ndataset/train/img0044.jpg 312,321,393,378,0 0,248,75,314,0 \\ndataset/train/img0045.jpg 321,326,398,375,0 \\ndataset/train/img0046.jpg 320,325,406,389,0 \\ndataset/train/img0047.jpg 326,330,406,386,0 \\ndataset/train/img0048.jpg 157,50,204,143,0 \\ndataset/train/img0052.jpg 350,155,384,202,0 296,252,339,281,0 347,252,384,286,0 \\ndataset/train/img0053.jpg 183,233,220,262,0 \\ndataset/train/img0054.jpg 192,234,222,271,0 378,250,410,277,0 \\ndataset/train/img0055.jpg 164,41,215,87,0 188,241,221,273,0 \\ndataset/train/img0056.jpg 148,222,184,260,0 \\ndataset/train/img0057.jpg 257,234,315,322,0 329,298,412,358,0 243,319,322,392,0 \\ndataset/train/img0058.jpg 374,244,420,282,0 \\ndataset/train/img0059.jpg 286,80,354,140,0 \\ndataset/train/img0069.jpg 136,2,178,75,0 \\ndataset/train/img0070.jpg 132,319,208,389,0 24,335,104,387,0 16,295,92,350,0 \\ndataset/train/img0071.jpg 282,286,362,354,0 \\ndataset/train/img0073.jpg 336,291,405,387,0 \\ndataset/train/img0079.jpg 171,274,265,338,0 82,338,172,386,0 5,242,86,307,0 0,296,79,361,0 \\ndataset/train/img0085.jpg 80,242,148,327,0 \\ndataset/train/img0088.jpg 199,234,265,330,0 160,337,246,390,0 \\ndataset/train/img0089.jpg 272,330,349,420,0 \\ndataset/train/img0090.jpg 300,330,374,410,0 232,342,303,414,0 \\ndataset/train/img0091.jpg 300,335,368,410,0 236,341,304,417,0 \\ndataset/train/img0092.jpg 295,329,360,400,0 244,319,284,409,0 \\ndataset/train/img0093.jpg 298,294,356,382,0 242,296,300,372,0 196,341,259,398,0 \\ndataset/train/img0094.jpg 194,227,256,327,0 \\ndataset/train/img0095.jpg 162,338,246,399,0 255,322,313,386,0 257,281,336,340,0 \\ndataset/train/img0096.jpg 312,205,381,272,0 \\ndataset/train/img0101.jpg 72,138,120,225,0 235,225,313,297,0 197,279,292,368,0 72,325,157,375,0 152,318,210,384,0 \\ndataset/train/img0102.jpg 246,233,315,299,0 222,286,288,362,0 144,311,207,385,0 71,321,153,374,0 \\ndataset/train/img0103.jpg 256,217,321,297,0 238,287,305,369,0 174,256,237,327,0 112,295,189,374,0 160,326,230,389,0 \\ndataset/train/img0104.jpg 243,193,321,244,0 241,282,301,363,0 189,248,261,318,0 128,278,197,346,0 156,316,228,382,0 \\ndataset/train/img0105.jpg 133,268,216,322,0 179,309,249,382,0 216,253,288,301,0 248,290,339,352,0 16,331,100,386,0 \\ndataset/train/img0106.jpg 149,225,241,290,0 225,254,308,302,0 216,299,301,357,0 273,314,334,393,0 \\ndataset/train/img0107.jpg 212,197,294,258,0 248,240,332,294,0 244,292,326,364,0 190,297,271,379,0 364,318,412,394,0 \\ndataset/train/img0108.jpg 30,326,84,388,0 230,250,292,324,0 280,206,353,278,0 282,267,348,349,0 \\ndataset/train/img0109.jpg 146,267,214,358,0 220,222,289,322,0 284,269,352,360,0 \\ndataset/train/img0110.jpg 120,150,182,206,0 175,240,238,317,0 148,322,242,382,0 4,322,75,377,0 \\ndataset/train/img0111.jpg 115,154,191,214,0 2,329,71,385,0 156,329,239,383,0 \\ndataset/train/img0112.jpg 113,149,207,215,0 4,330,82,384,0 176,294,248,381,0 \\ndataset/train/img0113.jpg 122,155,215,215,0 3,333,90,381,0 170,302,252,385,0 237,341,312,418,0 323,362,412,414,0 \\ndataset/train/img0114.jpg 289,206,361,286,0 224,329,298,392,0 \\ndataset/train/img0115.jpg 215,78,280,146,0 138,252,189,342,0 156,338,230,386,0 267,290,340,375,0 \\ndataset/train/img0116.jpg 201,82,254,124,0 121,250,204,318,0 142,331,229,391,0 285,322,360,382,0 \\ndataset/train/img0117.jpg 136,248,197,320,0 117,325,226,386,0 \\ndataset/train/img0118.jpg 28,157,58,207,0 23,251,62,286,0 \\ndataset/train/img0119.jpg 226,214,252,243,0 \\ndataset/train/img0120.jpg 120,226,144,254,0 57,251,90,278,0 \\ndataset/train/img0121.jpg 67,246,104,275,0 145,250,176,274,0 334,225,389,282,0 \\ndataset/train/img0122.jpg 156,260,196,290,0 \\ndataset/train/img0123.jpg 310,64,385,199,0 6,298,97,363,0 \\ndataset/train/img0124.jpg 52,244,120,288,0 201,125,232,186,0 218,290,281,339,0 \\ndataset/train/img0125.jpg 322,324,387,380,0 \\ndataset/train/img0126.jpg 340,354,409,414,0 165,16,251,97,0 \\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "_iUQgfOSUz_G",
        "colab_type": "code",
        "outputId": "37cecbb9-9e64-46f6-c389-e0a846e0faba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "cell_type": "code",
      "source": [
        "!cp -a \"/content/gdrive/My Drive/pszczoly/images/.\" \"dataset/train\"\n",
        "!ls \"dataset/train\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img0000.jpg  img0022.jpg  img0044.jpg  img0066.jpg  img0088.jpg  img0110.jpg\n",
            "img0001.jpg  img0023.jpg  img0045.jpg  img0067.jpg  img0089.jpg  img0111.jpg\n",
            "img0002.jpg  img0024.jpg  img0046.jpg  img0068.jpg  img0090.jpg  img0112.jpg\n",
            "img0003.jpg  img0025.jpg  img0047.jpg  img0069.jpg  img0091.jpg  img0113.jpg\n",
            "img0004.jpg  img0026.jpg  img0048.jpg  img0070.jpg  img0092.jpg  img0114.jpg\n",
            "img0005.jpg  img0027.jpg  img0049.jpg  img0071.jpg  img0093.jpg  img0115.jpg\n",
            "img0006.jpg  img0028.jpg  img0050.jpg  img0072.jpg  img0094.jpg  img0116.jpg\n",
            "img0007.jpg  img0029.jpg  img0051.jpg  img0073.jpg  img0095.jpg  img0117.jpg\n",
            "img0008.jpg  img0030.jpg  img0052.jpg  img0074.jpg  img0096.jpg  img0118.jpg\n",
            "img0009.jpg  img0031.jpg  img0053.jpg  img0075.jpg  img0097.jpg  img0119.jpg\n",
            "img0010.jpg  img0032.jpg  img0054.jpg  img0076.jpg  img0098.jpg  img0120.jpg\n",
            "img0011.jpg  img0033.jpg  img0055.jpg  img0077.jpg  img0099.jpg  img0121.jpg\n",
            "img0012.jpg  img0034.jpg  img0056.jpg  img0078.jpg  img0100.jpg  img0122.jpg\n",
            "img0013.jpg  img0035.jpg  img0057.jpg  img0079.jpg  img0101.jpg  img0123.jpg\n",
            "img0014.jpg  img0036.jpg  img0058.jpg  img0080.jpg  img0102.jpg  img0124.jpg\n",
            "img0015.jpg  img0037.jpg  img0059.jpg  img0081.jpg  img0103.jpg  img0125.jpg\n",
            "img0016.jpg  img0038.jpg  img0060.jpg  img0082.jpg  img0104.jpg  img0126.jpg\n",
            "img0017.jpg  img0039.jpg  img0061.jpg  img0083.jpg  img0105.jpg  img0127.jpg\n",
            "img0018.jpg  img0040.jpg  img0062.jpg  img0084.jpg  img0106.jpg  img0128.jpg\n",
            "img0019.jpg  img0041.jpg  img0063.jpg  img0085.jpg  img0107.jpg  img0129.jpg\n",
            "img0020.jpg  img0042.jpg  img0064.jpg  img0086.jpg  img0108.jpg  img0130.jpg\n",
            "img0021.jpg  img0043.jpg  img0065.jpg  img0087.jpg  img0109.jpg  img0131.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "329y104dUR_G",
        "colab_type": "code",
        "outputId": "7ec9ae13-b554-49a7-f4a0-c3ecf938f49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4051
        }
      },
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2018-12-28 20:46:06.581075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-12-28 20:46:06.581562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2018-12-28 20:46:06.581605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2018-12-28 20:46:06.995710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-12-28 20:46:06.995761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2018-12-28 20:46:06.995784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2018-12-28 20:46:06.996050: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2018-12-28 20:46:06.996110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Create Tiny YOLOv3 model with 6 anchors and 1 classes.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_10 due to mismatch in shape ((1, 1, 512, 18) vs (255, 512, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_10 due to mismatch in shape ((18,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_13 due to mismatch in shape ((1, 1, 256, 18) vs (30, 256, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_13 due to mismatch in shape ((18,) vs (30,)).\n",
            "  weight_values[i].shape))\n",
            "Load weights model_data/tiny_yolo_weights.h5.\n",
            "Freeze the first 42 layers of total 44 layers.\n",
            "Train on 77 samples, val on 8 samples, with batch size 32.\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 10s 5s/step - loss: 1172.0294 - val_loss: 1018.7349\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 955.9505 - val_loss: 830.6656\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 767.0034 - val_loss: 640.6251\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 621.1516 - val_loss: 521.1974\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 495.0079 - val_loss: 412.4746\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 386.0179 - val_loss: 352.9949\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 318.8750 - val_loss: 295.6845\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 268.7912 - val_loss: 233.2334\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 235.0593 - val_loss: 206.2589\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 6s 3s/step - loss: 199.2338 - val_loss: 161.3438\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 170.2723 - val_loss: 158.7408\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 149.4900 - val_loss: 134.3949\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 132.8225 - val_loss: 115.5914\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 120.7707 - val_loss: 112.8485\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 113.5395 - val_loss: 99.7367\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 104.9174 - val_loss: 91.9344\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 92.3025 - val_loss: 76.8941\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 88.2164 - val_loss: 79.4635\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 85.0158 - val_loss: 75.7732\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 72.3861 - val_loss: 75.9730\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 80.8179 - val_loss: 63.1108\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 73.1044 - val_loss: 67.1259\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 68.0403 - val_loss: 61.7558\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 68.1827 - val_loss: 60.4899\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 65.0860 - val_loss: 58.0974\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 65.7145 - val_loss: 58.0157\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 62.3074 - val_loss: 54.6713\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 62.5792 - val_loss: 52.2957\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 63.4307 - val_loss: 56.1435\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 58.6714 - val_loss: 54.6378\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 56.8524 - val_loss: 46.1539\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 57.0326 - val_loss: 50.2933\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 54.1043 - val_loss: 47.6416\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 53.2007 - val_loss: 49.6769\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 55.4001 - val_loss: 45.4589\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 49.5146 - val_loss: 46.2792\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 49.6521 - val_loss: 44.3789\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 51.9291 - val_loss: 46.2669\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 52.6550 - val_loss: 44.6095\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 48.1360 - val_loss: 42.4436\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 49.0352 - val_loss: 40.2705\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 49.6352 - val_loss: 41.6879\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 45.6633 - val_loss: 38.5424\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 47.5582 - val_loss: 41.3587\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 44.4366 - val_loss: 38.3120\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 46.2259 - val_loss: 38.7369\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 46.6722 - val_loss: 42.4094\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 44.0252 - val_loss: 37.4550\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 44.5208 - val_loss: 39.3100\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 5s 3s/step - loss: 40.5824 - val_loss: 39.2815\n",
            "Unfreeze all of the layers.\n",
            "Train on 77 samples, val on 8 samples, with batch size 32.\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 12s 6s/step - loss: 42.9528 - val_loss: 33.6065\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 34.8372 - val_loss: 28.7185\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 32.7006 - val_loss: 26.5415\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 31.6921 - val_loss: 27.1156\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 30.7238 - val_loss: 23.6095\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 27.6952 - val_loss: 22.7951\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 7s 3s/step - loss: 27.3670 - val_loss: 20.9130\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 7s 3s/step - loss: 25.8859 - val_loss: 21.3677\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 7s 3s/step - loss: 26.0016 - val_loss: 21.1199\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 7s 3s/step - loss: 26.2942 - val_loss: 20.2285\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 24.5886 - val_loss: 21.8811\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 25.1787 - val_loss: 22.0833\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 25.1565 - val_loss: 18.2479\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 22.5874 - val_loss: 19.0851\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 23.4806 - val_loss: 18.1457\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 27.3137 - val_loss: 18.8567\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 24.2091 - val_loss: 18.5741\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 23.7537 - val_loss: 18.9443\n",
            "\n",
            "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.0848 - val_loss: 20.0776\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 21.8847 - val_loss: 18.2556\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 22.8792 - val_loss: 18.6467\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 21.2249 - val_loss: 18.5056\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 25.1841 - val_loss: 18.3368\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 22.6202 - val_loss: 19.2484\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.6887 - val_loss: 18.0489\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.4041 - val_loss: 18.8889\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.4636 - val_loss: 19.7424\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.1232 - val_loss: 19.8806\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.7413 - val_loss: 19.6915\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.6725 - val_loss: 17.9552\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 23.6869 - val_loss: 16.8841\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 23.5108 - val_loss: 19.9751\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 23.6790 - val_loss: 19.3540\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 23.3182 - val_loss: 20.8677\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 22.2295 - val_loss: 19.7141\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 22.8159 - val_loss: 18.3919\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 23.6608 - val_loss: 19.4528\n",
            "\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 20.3166 - val_loss: 19.6165\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 24.4266 - val_loss: 19.1915\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 6s 3s/step - loss: 22.1795 - val_loss: 19.0678\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 5s 3s/step - loss: 25.1625 - val_loss: 19.6216\n",
            "Epoch 00091: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}